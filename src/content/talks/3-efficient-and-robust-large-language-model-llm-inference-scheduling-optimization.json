{
  "title": "Efficient and Robust Large Language Model (LLM) Inference\n\t\t\t\t\t\t\t\tScheduling\n\t\t\t\t\t\t\t\tOptimization",
  "abstract": "We study the problem of optimizing Large Language Model (LLM) inference scheduling to minimize total completion time. LLM inference is an online and multi-task service process and also heavily energy consuming by which a pre-trained LLM processes input requests and generates output tokens sequentially. Therefore, it is vital to improve its scheduling efficiency and reduce the power consumption while a great amount of prompt requests are arriving. There are two key challenges: (i) each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. We show that minimizing total completion time is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios are unbounded. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. (ii) the output length, which critically impacts memory usage and processing time, is unknown. We first design a conservative algorithm, Amax, which schedules requests based on the upper bound of predicted output lengths to prevent memory overflow. However, this approach is overly conservative: as prediction accuracy decreases, performance degrades significantly due to potential overestimation. To overcome this limitation, we propose Amin, an adaptive algorithm that initially treats the predicted lower bound as the output length and dynamically refines this estimate during inferencing. We prove that Amin achieves a log-scale competitive ratio.",
  "presenter": [
    {
      "name": "Zijie Zhou",
      "affiliation": "IEDA, HKUST",
      "photo": "Zijie_Zhou.jpg",
      "link": "https://sites.google.com/view/zijiezhou/",
      "continent": "Asia"
    }
  ],
  "invitedBy": "Guohua Wan",
  "datePrague": "2025-10-29",
  "keywords": "Scheduling, Optimization for LLM inference, Approximation online algorithms",
  "pdf": "/presentations/SchedulingSeminar_ZijieZhou.pdf",
  "video": "https://scheduling-seminar.iid.ciirc.cvut.cz/videos/25_3_Fall/25_10_29%20Zijie%20Zhou%20(IEDA,%20HKUST)%20_%20Efficient%20and%20Robust%20LLM%20Scheduling.mp4",
  "youtubeEmbed": "https://www.youtube.com/embed/obTtEShQKXc?si=0RYly4VpxVZ_-nLV",
  "status": "past"
}